{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import path\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('tensorflow version:',tf.__version__)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist_th = []\n",
    "flist_tr = []\n",
    "tot_rec = 0\n",
    "threat_rec = 0\n",
    "traffic_rec = 0\n",
    "discard_rec = 0\n",
    "with open(\"palo_alto_sample_300.txt\",\"r\") as file:\n",
    "    file_th = open(\"threat_file.txt\",\"a\")\n",
    "    file_th.truncate(0)\n",
    "    file_tr = open(\"traffic_file.txt\",\"a\")\n",
    "    file_tr.truncate(0)\n",
    "    for fline in file.readlines():\n",
    "        tot_rec+=1\n",
    "        fline = re.sub(r'[^a-zA-Z0-9\\n\\/.:,_\\s-]','',fline).lower()\n",
    "        flist = fline.split(\",\")\n",
    "        if (flist[3] == 'threat'):\n",
    "            flist_th.append(flist)\n",
    "            file_th.write(fline)\n",
    "            threat_rec+=1\n",
    "        elif(flist[3] == 'traffic'):\n",
    "            flist_tr.append(flist)\n",
    "            file_tr.write(fline)\n",
    "            traffic_rec+=1\n",
    "        else:\n",
    "            discard_rec+=1\n",
    "file_th.close()\n",
    "file_tr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total records:\",tot_rec)\n",
    "print(\"Threat records:\",threat_rec)\n",
    "print(\"Traffic records:\",traffic_rec)\n",
    "print(\"Discarded records:\",discard_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_tag = ['Header','Received Time','Serial Number','Log Type','Subtype','FUTURE USE','Generated Time','Source IP',\n",
    "              'Destination IP','NAT Source IP','NAT Destination IP','Rule Name','Source User','Destination User',\n",
    "              'Application','Virtual system','Source zone','Destination zone','Ingress Interface','Egress Interface',\n",
    "              'Log forwarding profile','FUTURE USE','Session ID','Repeat count','Source Port','Destination Port',\n",
    "              'NAT Source Port','NAT Destination Port','Flags','IP Protocol','Action','Miscellaneous','Threat ID',\n",
    "              'Category','Severity','Direction','Sequence Number','Action Flags','Source Location','Destination Location ',\n",
    "              'FUTURE USE','Content Type','PCAP_ID','Filedigest','Cloud','URL Index','User Agent','File Type',\n",
    "              'X-Forwarded-For','Referer','Sender','Subject','Recepient','Report ID','Device Group Hierarchy Level 1',\n",
    "              'Device Group Hierarchy Level 2','Device Group Hierarchy Level 3','Device Group Hierarchy Level 4',\n",
    "              'Virtual System Name','Device Name','FUTURE USE']\n",
    "\n",
    "threat_tag_ = ['HDR-M HDR-D HDR-T HDR-IP HDR-M HDR-D HDR-T HDR-DN HDR-CNT','RCVD-DT RCVD-TM','SRNO','LTYP','SBTYP','FTRUSE',\n",
    "              'GNRT-DT GNRT-TM','SRCIP','DSTIP','NATSIP','NATDIP','RULNM','SUSR','DUSR']\n",
    "\n",
    "threat_df = pd.DataFrame(columns=['Sentence_Tag_Pair'])\n",
    "\n",
    "with open(\"threat_file.txt\",\"r\") as file_th:\n",
    "    l_ctr = 0\n",
    "    for line in file_th:\n",
    "        line_lst = line.split(\",\")\n",
    "        line_list=[]\n",
    "        for i in range(0,len(threat_tag_)):\n",
    "            line_tup = (line_lst[i],threat_tag_[i])\n",
    "            line_list.append(line_tup)\n",
    "        threat_df['Sentence_Tag_Pair'] = threat_df['Sentence_Tag_Pair'].astype('object')\n",
    "        threat_df.at[l_ctr,'Sentence_Tag_Pair'] = line_list\n",
    "        l_ctr+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_tag = ['Header','Received Time','Serial Number','Log Type','Subtype','FUTURE USE','Generated Time',\n",
    "                'Source IP','Destination IP','NAT Source IP','NAT Destination IP','Rule Name','Source User',\n",
    "                'Destination User','Application','Virtual system','Source zone','Destination zone','Ingress Interface',\n",
    "                'Egress Interface','Log forwarding profile','FUTURE USE','Session ID','Repeat count','Source Port',\n",
    "                'Destination Port','NAT Source Port','NAT Destination Port','Flags','IP Protocol','Action',\n",
    "                'Bytes','Bytes Sent','Bytes Received','Packets','Start Time','Elapse Time','Category',\n",
    "                'FUTURE USE','Sequence Number','Action Flag','Source Location','Destination Location',\n",
    "                'FUTURE USE','Packets Sent','Packets Received','Session End Reason',\n",
    "                'Device Group Hierarchy Level 1','Device Group Hierarchy Level 2',\n",
    "                'Device Group Hierarchy Level 3','Device Group Hierarchy Level 4','Virtual Sytem name',\n",
    "                'Device Name','Action Source']\n",
    "\n",
    "traffic_tag_ = ['HDR-M HDR-D HDR-T HDR-IP HDR-M HDR-D HDR-T HDR-DN HDR-CNT','RCVD-DT RCVD-TM','SRNO','LTYP','SBTYP','FTRUSE',\n",
    "              'GNRT-DT GNRT-TM','SRCIP','DSTIP','NATSIP','NATDIP','RULNM','SUSR','DUSR']\n",
    "\n",
    "traffic_df = pd.DataFrame(columns=['Sentence_Tag_Pair'])\n",
    "\n",
    "with open(\"traffic_file.txt\",\"r\") as file_tr:\n",
    "    l_ctr = 0\n",
    "    for line in file_tr:\n",
    "        line_lst = line.split(\",\")\n",
    "        line_list=[]\n",
    "        for i in range(0,len(traffic_tag_)):\n",
    "            line_tup = (line_lst[i],traffic_tag_[i])\n",
    "            line_list.append(line_tup)\n",
    "        traffic_df['Sentence_Tag_Pair'] = traffic_df['Sentence_Tag_Pair'].astype('object')\n",
    "        traffic_df.at[l_ctr,'Sentence_Tag_Pair'] = line_list\n",
    "        l_ctr+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([threat_df,traffic_df],ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentence_Tag_Pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentence']=df['Sentence_Tag_Pair'].apply(lambda sentence:\" \".join([s[0] for s in sentence if s[0] != '']))\n",
    "\n",
    "df['Tag']=df['Sentence_Tag_Pair'].apply(lambda sentence:\" \".join([s[1] for s in sentence if s[0] != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenised_sentences']=df['Sentence'].apply(lambda x:x.split())\n",
    "df['tag_list']=df['Tag'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_sentence']=df['tokenised_sentences'].apply(lambda x:len(x))\n",
    "df['len_tag']=df['tag_list'].apply(lambda x:len(x))\n",
    "df['is_equal']=df.apply(lambda row:1 if row['len_sentence']==row['len_tag'] else 0,axis=1)\n",
    "df['is_equal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df=df[df['is_equal']!=0]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list=df['Sentence'].tolist()\n",
    "tags_list=df['tag_list'].tolist()\n",
    "\n",
    "print(\"Number of Sentences in the Data \",len(sentences_list))\n",
    "print(\"Are number of Sentences and Tag list equal? \",len(sentences_list)==len(tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tokenising the sentences\n",
    "\n",
    "We have to create a vocab of words and then each word should be assigned an unique identifier. Keras provides an Tokeniser API, which can be used for this purpose. It will both tokenise and encode sentences.\n",
    "\n",
    "Also, since different sentences are of different length, we will have to pad the input sequence to the largest sentence length.Keras provides pad_sequences function to achieve the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser= tf.keras.preprocessing.text.Tokenizer(lower=False,filters='')\n",
    "tokeniser.fit_on_texts(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokeniser.word_index\n",
    "vocab_size = len(word_index) +1\n",
    "print(\"Vocab size of Tokeniser \",vocab_size) ## Adding one since 0 is reserved for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a space into word_index \n",
    "#this can be later used in testing, when the new word is not found in the word_index\n",
    "#word_index[\" \"] = max(word_index.values())+1\n",
    "#print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word={v: k for k, v in word_index.items()}\n",
    "print(index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_vectors = dict()\n",
    "glove_dir = \"C://Users//anarayan//Anaconda3//glove.6B\"\n",
    "glove_file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = \"utf-8\")\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vectors = np.asarray(values[1:], dtype='float32')\n",
    "    glove_vectors[word] = vectors\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(glove_vectors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_vectors.get('you').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_vectors.get('2020/09/15') #word vectors not present for date,time,ip addr, and other log literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_vector_matrix = np.zeros((vocab_size,100))\n",
    "\n",
    "word_nt_fnd_cnt = 0\n",
    "for word, index in word_index.items():\n",
    "    vector = glove_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        word_vector_matrix[index] = vector\n",
    "    else:\n",
    "        word_nt_fnd_cnt +=1\n",
    "print(word_nt_fnd_cnt)\n",
    "#out of 86 words, 78 words are not present in GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot use GloVe vector as it doesnt have words defined for Machine Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extracting word and its weights from GloVe pre-trained vector \n",
    "#Cannot use GloVe vector as it doesnt have words defined for Machine Logs\n",
    "\n",
    "embedding_index = {}\n",
    "embedding_dim = 100\n",
    "glove_dir = \"C://Users//anarayan//Anaconda3//glove.6B\"\n",
    "glove_file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = \"utf-8\")\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeff = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coeff\n",
    "glove_file.close()\n",
    "\n",
    "#Cannot use GloVe vector as it doesnt have words defined for Machine Logs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#Cannot use GolVe vector as it doesnt have words defined for Machine Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "#word_vec_model = Word2Vec.load('word_vec_model.bin')\n",
    "#print(word_vec_model)\n",
    "#print(word_vec_model.wv.get_vector('sep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "f = open(os.path.join('', 'palo_alto_300_100d.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    #print('line :',line)\n",
    "    values = line.split()\n",
    "    #print('values :',values)\n",
    "    word = values[0]\n",
    "    #print('word :',word)\n",
    "    coeff = np.asarray(values[1:], dtype='float32')\n",
    "    #print('coeff :',coeff)\n",
    "    embedding_index[word] = coeff\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    print('word :',word)\n",
    "    try:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        print('embedding_vector :',embedding_matrix[i])\n",
    "    except KeyError: \n",
    "        embedding_matrix[i] = np.zeros((1,embedding_dim))\n",
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index.get('sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentence=tokeniser.texts_to_sequences(sentences_list)\n",
    "print(\"First Original Sentence \",sentences_list[0])\n",
    "print(\"First Encoded Sentence \",encoded_sentence[0])\n",
    "print(\"Is Length of Original Sentence Same as Encoded Sentence? \",len(sentences_list[0].split())==len(encoded_sentence[0]))\n",
    "print(\"Length of First Sentence \",len(encoded_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_tag_list = []\n",
    "for i in range(len(df)):\n",
    "    for x in range(len(df['tag_list'][i])):\n",
    "        unq_tag_list.append(df['tag_list'][i][x])\n",
    "\n",
    "tags=list(set(unq_tag_list))\n",
    "print(tags)\n",
    "# Add Unknown Tag into the List\n",
    "tags.append('UNKWN')\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags=len(tags)\n",
    "print(\"Number of Tags \",num_tags)\n",
    "\n",
    "tags_map={tag:i for i,tag in enumerate(tags)}\n",
    "print(\"Tags Map \",tags_map)\n",
    "\n",
    "reverse_tags_map={v: k for k, v in tags_map.items()}\n",
    "print(\"Reverse Tags Map \",reverse_tags_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tags=[[tags_map[w] for w in tag] for tag in tags_list]\n",
    "print(\"First Sentence :\",sentences_list[0])\n",
    "print('First Sentence Original Tags :',tags_list[0])\n",
    "print(\"First Sentence Encoded Tags :\",encoded_tags[0])\n",
    "print(\"Is length of Original Tags and Encoded Tags same? \",len(tags_list[0])==len(encoded_tags[0]))\n",
    "print(\"Length of Tags for First Sentence :\",len(encoded_tags[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length=max([len(s.split()) for s in sentences_list])\n",
    "print(max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to pad sentence tokens with max tokens from sentence. During testing, the sentence can grow upto 2**7 = 128\n",
    "#Calculate ceiling integer value of the sqrt of max_sequence_length \n",
    "n_sqrt = math.sqrt(max_sentence_length)\n",
    "print(n_sqrt)\n",
    "n_sqrt_ceil = math.ceil(n_sqrt)\n",
    "print(n_sqrt_ceil)\n",
    "inp_layer = 2**n_sqrt_ceil\n",
    "print(inp_layer)\n",
    "inp_layer = 128 #for testing needs, 128 is set as max length of a sentence\n",
    "print(inp_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Length is set in power of 2, so that padding can be done for empty token and further word embedding is consistent\n",
    "max_len=inp_layer\n",
    "\n",
    "padded_encoded_sentences=pad_sequences(maxlen=max_len,sequences=encoded_sentence,padding=\"post\",value=0)\n",
    "padded_encoded_tags=pad_sequences(maxlen=max_len,sequences=encoded_tags,padding=\"post\",value=tags_map['UNKWN'])\n",
    "\n",
    "print(\"Shape of Encoded Sentence \",padded_encoded_sentences.shape)\n",
    "print(\"Shape of Encoded Labels \",padded_encoded_tags.shape)\n",
    "\n",
    "print(\"First Encoded Sentence Without Padding \",encoded_sentence[0])\n",
    "print(\"First Encoded Sentence with padding \",padded_encoded_sentences[0])\n",
    "print(\"First Sentence Encoded Label without Padding \",encoded_tags[0])\n",
    "print(\"First Sentence Encoded Label with Padding \",padded_encoded_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_encoded_sentences.shape)\n",
    "padded_encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_encoded_tags.shape)\n",
    "padded_encoded_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical encoding for tags present in each sentence = no. of sent X max_len tags X total no. of tags\n",
    "#(since, total of 20 unique encodings present in dataset)\n",
    "#target shape = (total_no_of _sentence,total_no_of_tags,no_of_unique_tags)\n",
    "target= [to_categorical(i,num_classes = num_tags) for i in padded_encoded_tags]\n",
    "print(\"Shape of Labels  after converting to Categorical for first sentence \",target[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into Train,Test and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val_test,y_train,y_val_test = train_test_split(padded_encoded_sentences,target,test_size = 0.3,random_state=42)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val_test,y_val_test,test_size = 0.2,random_state=42)\n",
    "\n",
    "# X_train shape = (5,24)\n",
    "# y_train shape = (5,24,20)\n",
    "print(\"Input Train Data Shape \",X_train.shape)\n",
    "print(\"Train Labels Length \",len(y_train))\n",
    "\n",
    "# X_test shape = (1,24)\n",
    "# y_test shape = (1,24,20)\n",
    "print(\"Input Test Data Shape \",X_test.shape)\n",
    "print(\"Test Labels Length \",len(y_test))\n",
    "\n",
    "# X_val shape = (2,24)\n",
    "# y_val shape = (2,24,20)\n",
    "print(\"Input Validation Data Shape \",X_val.shape)\n",
    "print(\"Validation Labels Length \",len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Sentence in Training Data :\\n\",X_train[0]) #represents the sentence encoding using tokeniser (24,)\n",
    "print(\"First sentence Label :\\n\",y_train[0]) #represents the tag encoding using tokeniser + encoding (24,20)\n",
    "print(\"Shape of First Sentence -Train :\",X_train[0].shape)\n",
    "print(\"Shape of First Sentence Label  -Train :\",y_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import LSTM,Embedding,Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "from numpy.random import seed\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_dim=100                               #based on custome word2vec embedding\n",
    "#vocab_size=len(tokeniser.word_index)+1          #total token in the dataset\n",
    "#max_len=24                                      #max token in a sentence (23) + padding (1) \n",
    "#num_tags=len(tags)                              #number of tags in the dataset\n",
    "\n",
    "print(\"Embedding Dimension :\",embedding_dim)\n",
    "print(\"Vocabulary Size :\",vocab_size)\n",
    "print(\"Maximum length of sentence :\",max_len)\n",
    "print(\"Unique Number of Tags :\",num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM architecture - \n",
    "#        Input: Sentence encoded in tokens: (?,24)\n",
    "#        Embedding:  (?,24) --> (?,24,64)\n",
    "#        BLSTM:  (?,24,64) --> (?,24,128)\n",
    "#        LSTM:  (?,24,128) --> (?,24,256)\n",
    "\n",
    "#        Output: (?,24,20) (20 no. of tags defined in this dataset)\n",
    "\n",
    "input_word = Input(shape = (max_len,))\n",
    "\n",
    "#using custom trained embedding \n",
    "#model = Embedding(input_dim=vocab_size+1,output_dim=embedding_dim,input_length=max_len)(input_word)\n",
    "model = Embedding(input_dim=vocab_size,output_dim=embedding_dim,weights=[embedding_matrix],\n",
    "                  input_length=max_len,trainable=False)(input_word)\n",
    "\n",
    "model = Bidirectional(LSTM(units=embedding_dim,return_sequences=True,recurrent_dropout=0.2))(model)\n",
    "\n",
    "model = LSTM(units=embedding_dim,return_sequences=True,recurrent_dropout=0.5)(model)\n",
    "\n",
    "out = TimeDistributed(Dense(num_tags,activation = 'softmax'))(model)\n",
    "\n",
    "#Trying to CRF layer -  but getting issues with model.fit \n",
    "#model = TimeDistributed(Dense(num_tags,activation = 'softmax'))(model)\n",
    "#crf = CRF(num_tags) #Conditional Random Field\n",
    "#out = crf(model)\n",
    "\n",
    "model = Model(input_word,out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "#Cannot run CRF layer due to model.fit issue\n",
    "#model.compile(optimizer= 'adam', loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "best_model_weight = 'blstm_ner_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_weight, save_best_only=True, save_weights_only=True)\n",
    "callback_list=[early_stopping, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(X_train,np.array(y_train),validation_data=(X_val,np.array(y_val)),batch_size = 4,epochs = 20, \n",
    "#                    callbacks=callbacks_list)\n",
    "#history = model.fit(X_train,np.array(y_train),validation_data=(X_val,np.array(y_val)),batch_size = 4,epochs = 30)\n",
    "history = model.fit(X_train,np.array(y_train),validation_data=(X_val,np.array(y_val)),batch_size=4,epochs=30,\n",
    "                    shuffle=True,callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"blstm-ner-model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"blstm-ner-model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the tokenizer using pickle\n",
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokeniser.pickle', 'wb') as tokens:\n",
    "    pickle.dump(tokeniser, tokens, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "#with open('tokeniser.pickle', 'rb') as tokens:\n",
    "#    tokeniser = pickle.load(tokens)\n",
    "\n",
    "#Saving the tags_map using pickle\n",
    "with open('tags_map.pickle', 'wb') as tags:\n",
    "    pickle.dump(tags_map, tags, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# loading\n",
    "#with open('tags_map.pickle', 'rb') as tags:\n",
    "#    tags_map = pickle.load(tags)\n",
    "\n",
    "#Saving the embedding_index as json\n",
    "#with open(\"embedding_index.json\", \"w\") as json:\n",
    "#    pickle.dump(embedding_index, json)\n",
    "# loading\n",
    "#with open('tags_map.pickle', 'rb') as tags:\n",
    "#    tags_map = pickle.load(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will have to remove the padded portion and identify the accuracy. For this, for every test data, let us create a dataframe wuth the tokens and the actual and predicted value - and use it to calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('blstm-ner-model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"blstm-ner-model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=loaded_model.predict(X_test) ## Predict using model on Test Data\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePredictions(test_data,preds,actual):\n",
    "    print(\"Shape of Test Data Array\",test_data.shape)\n",
    "    y_actual=np.argmax(np.array(actual),axis=2)\n",
    "    print(\"y_actual :\",y_actual)\n",
    "    y_pred=np.argmax(preds,axis=2)\n",
    "    print(\"y_pred :\",y_pred)\n",
    "    num_test_data=test_data.shape[0]\n",
    "    print(\"Number of Test Data Points \",num_test_data)\n",
    "    data=pd.DataFrame()\n",
    "    df_list=[]\n",
    "    for i in range(num_test_data):\n",
    "        #print(\"Test Data :\", test_data[i])\n",
    "        test_str=list(test_data[i])\n",
    "        #print(\"Test Str :\", test_str)\n",
    "        df=pd.DataFrame()\n",
    "        df['test_tokens']=test_str\n",
    "        df['tokens']=df['test_tokens'].apply(lambda x:tokeniser.index_word[x] if x!=0 else '<PAD>')\n",
    "        df['actual_target_index']=list(y_actual[i])\n",
    "        df['pred_target_index']=list(y_pred[i])\n",
    "        df['actual_target_tag']=df['actual_target_index'].apply(lambda x:reverse_tags_map[x])\n",
    "        df['pred_target_tag']=df['pred_target_index'].apply(lambda x:reverse_tags_map[x])\n",
    "        df['id']=i+1\n",
    "        df_list.append(df)\n",
    "        #print(\"df list :\", df_list)\n",
    "    data=pd.concat(df_list)\n",
    "    #print(\"data :\", data)\n",
    "    pred_data=data[data['tokens']!='<PAD>']\n",
    "    #print(\"Pred data :\", pred_data)\n",
    "    accuracy=pred_data[pred_data['actual_target_tag']==pred_data['pred_target_tag']].shape[0]/pred_data.shape[0]\n",
    "    print(\"Accuracy :\", accuracy)\n",
    "    \n",
    "    \n",
    "    return pred_data,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data,accuracy=evaluatePredictions(X_test,preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pred_data['pred_target_tag'].tolist()\n",
    "y_actual=pred_data['actual_target_tag'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = pred_data[['actual_target_tag','pred_target_tag']]\n",
    "print(check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_actual,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Testing Purpose\n",
    "text1 = 'Oct 15 01:01:01 01.01.01.01 Oct 15 14:16:24 DC2-PA-3020.npi.int 1,2020/09/15 14:16:24,001801030168,SYSTEM,vpn,0,2020/09/15 14:16:24'\n",
    "text2 = 'Sep 15 14:11:57 192.168.199.245 Sep 15 14:16:24 DC2-PA-3020.npi.int 1,2020/09/15 14:16:23,001801030168,TRAFFIC,end,2304,2020/09/15 14:16:23,192.168.198.239,208.67.222.222,107.6.5.20,208.67.222.222,Internet-Egnyte,npi\\eu-support,,dns,vsys1,Transit_ASA,outside,ethernet1/5,ethernet1/1,Log_Forwarding,2020/09/15 14:16:23,110034,1,52911,53,34334,53,0x404019,udp,allow,370,121,249,2,2020/09/15 14:15:51,0,any,0,2978975507,0x0,192.168.0.0-192.168.255.255,United States,0,1,1,aged-out,0,0,0,0,,DC2-PA-3020,from-policy,,,0,,0,,N/A,0,0,0,0,6cfb47f4-da09-4589-b312-0dbff677b43d,0'\n",
    "text_lst = text2.replace(\",\", \" \")\n",
    "text_list = text_lst.split(\" \")\n",
    "\n",
    "encoded_text_list=tokeniser.texts_to_sequences(text_list)\n",
    "print(encoded_text_list)\n",
    "\n",
    "for i in range(len(encoded_text_list)):\n",
    "    if len(encoded_text_list[i]):\n",
    "        encoded_text_list[i] = encoded_text_list[i][0]\n",
    "    else:    \n",
    "        encoded_text_list[i] = word_index.get(\" \")\n",
    "\n",
    "print(len(encoded_text_list))\n",
    "print(encoded_text_list)\n",
    "print(max_len)\n",
    "for i in range(0,max_len):\n",
    "     if (len(encoded_text_list) <= i):\n",
    "        encoded_text_list.append(0)\n",
    "\n",
    "print(len(encoded_text_list))\n",
    "print(encoded_text_list)\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text_list = loaded_model.predict([encoded_text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePrediction(text_list,test_data,preds):\n",
    "    y_pred=np.argmax(preds,axis=2)\n",
    "    print(\"y_pred :\",y_pred)\n",
    "    print(text_list)\n",
    "    data=pd.DataFrame()\n",
    "    df_list=[]\n",
    "    print(len(test_data))\n",
    "    for i in range(len(test_data)):\n",
    "        print(\"Test Data :\", test_data[i])\n",
    "        test_str=list(test_data[i])\n",
    "        print(\"Test Str :\", test_str)\n",
    "        df=pd.DataFrame()\n",
    "        text_df=pd.DataFrame()\n",
    "        text_df['text_list']=text_list\n",
    "        df['test_tokens']=test_str\n",
    "        df['tokens_from_word_index']=df['test_tokens'].apply(lambda x:index_word[x] if x!=0 else '<PAD>')\n",
    "        df['pred_target_index']=list(y_pred[i])\n",
    "        df['pred_target_tag']=df['pred_target_index'].apply(lambda x:reverse_tags_map[x])\n",
    "        df['id']=i+1\n",
    "        df = pd.concat([text_df,df],axis=1)\n",
    "        df_list.append(df)\n",
    "    data=pd.concat(df_list)\n",
    "    pred_data=data[data['tokens_from_word_index']!='<PAD>']\n",
    "    \n",
    "    \n",
    "    return pred_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_text_list = generatePrediction(text_list,[encoded_text_list],pred_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_text_list[0:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
