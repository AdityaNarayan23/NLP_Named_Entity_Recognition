{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U spacy\n",
    "#pip install --user en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "#with open(\"sample.txt\", \"r\") as fh:\n",
    "#with open(\"hdfs-audit.log\", \"r\") as fh:\n",
    "with open(\"sample1.txt\", \"r\") as fh:\n",
    "    fstring = fh.readlines()\n",
    "print(len(fstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_lst=[]\n",
    "with open(\"sample1.txt\",\"r\") as fh:\n",
    "    for line in fh:\n",
    "        res = re.compile(r'(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})')\n",
    "        ent_tup=()\n",
    "        ent_lst=[]\n",
    "        file_tup=[]\n",
    "        ent_dict={\"entities\":[]}\n",
    "        for m in res.finditer(line):\n",
    "            ent_tup = (m.start(),m.end(),\"IP\")\n",
    "            ent_lst.append(ent_tup)\n",
    "        ent_dict[\"entities\"] = ent_lst\n",
    "        file_tup = (line,ent_dict)\n",
    "        file_lst.append(file_tup)        \n",
    "        #print(file_tup)\n",
    "print(len(file_lst))\n",
    "#print(file_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.5\n",
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm as en\n",
    "print(spacy.__version__)\n",
    "print(en.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.language import Language\n",
    "#@Language.component(\"info_component\")\n",
    "#def my_component(doc):\n",
    "#    print(f\"After tokenization, this doc has {len(doc)} tokens.\")\n",
    "#    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
    "#    if len(doc) < 10:\n",
    "#        print(\"This is a pretty short document.\")\n",
    "#    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "#print(source_nlp.pipe_names)\n",
    "##mer = source_nlp.get_pipe(\"ner\")\n",
    "##print(mer)\n",
    "#nlp = spacy.blank(\"en\")\n",
    "#nlp.add_pipe(\"ner\",source=source_nlp)\n",
    "#print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new pipe - \"print_info\" into nlp object\n",
    "#nlp.add_pipe(\"info_component\", name=\"print_info\", first=True)\n",
    "#print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
    "#doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x000001F28233D940>\n",
      "Australia 0 9 GPE\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp)\n",
    "doc = nlp(\"Australia\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "<spacy.pipeline.ner.EntityRecognizer object at 0x000001F284E33DC0>\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in file_lst:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        #print(ent[2])\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "disable_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "print(disable_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in iter 0 is {'ner': 40.1455352544494}\n",
      "Loss in iter 1 is {'ner': 12.038721134380337}\n",
      "Loss in iter 2 is {'ner': 0.04384283711739288}\n",
      "Loss in iter 3 is {'ner': 0.16801913500963014}\n",
      "Loss in iter 4 is {'ner': 0.0003263983282073047}\n",
      "Loss in iter 5 is {'ner': 0.0003230234689985609}\n",
      "Loss in iter 6 is {'ner': 0.0005080485768495294}\n",
      "Loss in iter 7 is {'ner': 1.7935567961747136e-05}\n",
      "Loss in iter 8 is {'ner': 3.517261583522815e-06}\n",
      "Loss in iter 9 is {'ner': 0.008232498792038332}\n",
      "Loss in iter 10 is {'ner': 0.00042963463771303616}\n",
      "Loss in iter 11 is {'ner': 0.0007365443208581817}\n",
      "Loss in iter 12 is {'ner': 1.0504157268138657e-06}\n",
      "Loss in iter 13 is {'ner': 3.331751463274324e-09}\n",
      "Loss in iter 14 is {'ner': 1.9195951561787953e-07}\n",
      "Loss in iter 15 is {'ner': 6.342007756991866e-08}\n",
      "Loss in iter 16 is {'ner': 6.2457642054939905e-06}\n",
      "Loss in iter 17 is {'ner': 9.028794868388489e-08}\n",
      "Loss in iter 18 is {'ner': 2.824638424029554e-05}\n",
      "Loss in iter 19 is {'ner': 1.1148561750112597e-08}\n",
      "Loss in iter 20 is {'ner': 4.084267168186536e-09}\n",
      "Loss in iter 21 is {'ner': 6.745065324578919e-07}\n",
      "Loss in iter 22 is {'ner': 1.9379793881210457e-05}\n",
      "Loss in iter 23 is {'ner': 1.0711993327834113e-07}\n",
      "Loss in iter 24 is {'ner': 2.1633445574838144e-08}\n",
      "Loss in iter 25 is {'ner': 8.47597527435183e-09}\n",
      "Loss in iter 26 is {'ner': 1.3918127292471436e-08}\n",
      "Loss in iter 27 is {'ner': 1.359214407496464e-09}\n",
      "Loss in iter 28 is {'ner': 2.3501561405982626e-05}\n",
      "Loss in iter 29 is {'ner': 1.66985875492113e-08}\n",
      "Loss in iter 30 is {'ner': 3.2858392649192848e-09}\n",
      "Loss in iter 31 is {'ner': 6.804173787797293e-09}\n",
      "Loss in iter 32 is {'ner': 4.900696634364004e-07}\n",
      "Loss in iter 33 is {'ner': 6.9802590225544485e-09}\n",
      "Loss in iter 34 is {'ner': 5.738819585393833e-09}\n",
      "Loss in iter 35 is {'ner': 8.131089543730632e-07}\n",
      "Loss in iter 36 is {'ner': 1.0500038930638392e-08}\n",
      "Loss in iter 37 is {'ner': 0.015842088622161316}\n",
      "Loss in iter 38 is {'ner': 0.00312510219842288}\n",
      "Loss in iter 39 is {'ner': 1.8026658427369438e-07}\n",
      "Loss in iter 40 is {'ner': 4.3683740018779024e-08}\n",
      "Loss in iter 41 is {'ner': 1.260289919927106e-05}\n",
      "Loss in iter 42 is {'ner': 1.1978782076707307e-06}\n",
      "Loss in iter 43 is {'ner': 5.7081655167632175e-09}\n",
      "Loss in iter 44 is {'ner': 3.8729540957548116e-10}\n",
      "Loss in iter 45 is {'ner': 2.157598813795821e-11}\n",
      "Loss in iter 46 is {'ner': 3.8246750171494305e-08}\n",
      "Loss in iter 47 is {'ner': 1.975804397731396e-08}\n",
      "Loss in iter 48 is {'ner': 0.07847356992828552}\n",
      "Loss in iter 49 is {'ner': 2.59046770888521e-05}\n",
      "Loss in iter 50 is {'ner': 1.1984561524970985e-08}\n",
      "Loss in iter 51 is {'ner': 5.7433372566500865e-08}\n",
      "Loss in iter 52 is {'ner': 7.969204373478466e-11}\n",
      "Loss in iter 53 is {'ner': 1.3468200681119314e-08}\n",
      "Loss in iter 54 is {'ner': 8.101052840592387e-08}\n",
      "Loss in iter 55 is {'ner': 2.0604953622203274e-09}\n",
      "Loss in iter 56 is {'ner': 1.398943314404274e-08}\n",
      "Loss in iter 57 is {'ner': 1.469557288457173e-07}\n",
      "Loss in iter 58 is {'ner': 1.328829141311629e-09}\n",
      "Loss in iter 59 is {'ner': 2.066315614826783e-12}\n",
      "Loss in iter 60 is {'ner': 2.036825984192572e-07}\n",
      "Loss in iter 61 is {'ner': 8.403331477938884e-10}\n",
      "Loss in iter 62 is {'ner': 1.5725801131183112e-10}\n",
      "Loss in iter 63 is {'ner': 9.962200250209975e-11}\n",
      "Loss in iter 64 is {'ner': 1.468593552740339e-10}\n",
      "Loss in iter 65 is {'ner': 3.491305499411081e-11}\n",
      "Loss in iter 66 is {'ner': 1.0223064435397017e-07}\n",
      "Loss in iter 67 is {'ner': 4.423063431954912e-12}\n",
      "Loss in iter 68 is {'ner': 5.665328241467902e-09}\n",
      "Loss in iter 69 is {'ner': 2.4503019165026462e-11}\n",
      "Loss in iter 70 is {'ner': 4.888062325765809e-11}\n",
      "Loss in iter 71 is {'ner': 3.377840363970086e-09}\n",
      "Loss in iter 72 is {'ner': 1.4876824040623072e-11}\n",
      "Loss in iter 73 is {'ner': 1.4764518877626623e-09}\n",
      "Loss in iter 74 is {'ner': 1.324519813071682e-07}\n",
      "Loss in iter 75 is {'ner': 5.3385378883889863e-11}\n",
      "Loss in iter 76 is {'ner': 1.24871240090102e-10}\n",
      "Loss in iter 77 is {'ner': 1.2056412132997477e-09}\n",
      "Loss in iter 78 is {'ner': 4.6754726461465665e-09}\n",
      "Loss in iter 79 is {'ner': 3.4703923223660173e-12}\n",
      "Loss in iter 80 is {'ner': 7.314298703224415e-12}\n",
      "Loss in iter 81 is {'ner': 1.1636625516403892e-09}\n",
      "Loss in iter 82 is {'ner': 1.2871927933665744e-07}\n",
      "Loss in iter 83 is {'ner': 6.018869685002824e-12}\n",
      "Loss in iter 84 is {'ner': 2.069558087052996e-10}\n",
      "Loss in iter 85 is {'ner': 1.0811448401178208e-10}\n",
      "Loss in iter 86 is {'ner': 5.541148315210689e-08}\n",
      "Loss in iter 87 is {'ner': 2.189775514159915e-09}\n",
      "Loss in iter 88 is {'ner': 1.648634194550661e-10}\n",
      "Loss in iter 89 is {'ner': 6.339523129034572e-10}\n",
      "Loss in iter 90 is {'ner': 3.187779410979436e-10}\n",
      "Loss in iter 91 is {'ner': 2.624977861840717e-11}\n",
      "Loss in iter 92 is {'ner': 1.0515960005491078e-08}\n",
      "Loss in iter 93 is {'ner': 6.785945673437676e-11}\n",
      "Loss in iter 94 is {'ner': 4.581130631342535e-12}\n",
      "Loss in iter 95 is {'ner': 2.612342258455192e-11}\n",
      "Loss in iter 96 is {'ner': 4.9952926171794364e-11}\n",
      "Loss in iter 97 is {'ner': 2.3148731803689785e-11}\n",
      "Loss in iter 98 is {'ner': 4.729419813604135e-12}\n",
      "Loss in iter 99 is {'ner': 1.2965154475903238e-08}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "\n",
    "with nlp.disable_pipes(*disable_pipes):\n",
    "    \n",
    "    optimizer = nlp.resume_training()\n",
    "    \n",
    "    for iteration in range(100):\n",
    "        \n",
    "        random.shuffle(file_lst)\n",
    "        losses={}\n",
    "        \n",
    "        for batch in minibatch(file_lst, size=5):\n",
    "            for text,annotation in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc,annotation)\n",
    "                #print(example)\n",
    "                nlp.update([example],losses=losses,drop=0.3,sgd=optimizer)\n",
    "        print(f'Loss in iter {iteration} is {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('10.45.149.37', 'IP'), ('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities [('13.24.199.197', 'IP')]\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n",
      "Entities []\n"
     ]
    }
   ],
   "source": [
    "for text, _ in file_lst:\n",
    "    #print(text)\n",
    "    doc=nlp(text)\n",
    "    print(\"Entities\",[(ent.text,ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.167.23.23\n",
      "192.167.23.23 33 46 IP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the IP which is causing issue in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    192.167.23.23\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IP</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp(\"the IP which is causing issue in 192.167.23.23\")\n",
    "for ent in doc.ents:\n",
    "    print(ent)\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "displacy.render(nlp(doc.text),style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
